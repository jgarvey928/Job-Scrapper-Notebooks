{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgarvey928/Colab_Notebooks/blob/main/Job_Scapper_Ext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# Install necessary libraries\n",
        "!pip install selenium nltk\n",
        "\n",
        "# Import necessary libraries\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "\n",
        "# Standard Library Imports\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "\n",
        "# Third-Party Library Imports\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import ngrams\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "# These downloads are important for text processing tasks later in the notebook\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# The following downloads might not be standard or needed, you might want to\n",
        "# verify if they are truly required for your analysis.\n",
        "nltk.download('punkt_tab') # Usually not needed for standard text processing\n",
        "nltk.download('averaged_perceptron_tagger') # Useful for pos_tagging\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Likely the same as the above or a specific English version"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBGsqHDMmyFe",
        "outputId": "1cb9e3e5-6abc-4368-de46-ef756cdb7bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.38.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio<1.0,>=0.31.0 (from selenium)\n",
            "  Downloading trio-0.32.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.10.5 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.11.12)\n",
            "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (4.15.0)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (3.11)\n",
            "Collecting outcome (from trio<1.0,>=0.31.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
            "  Downloading wsproto-1.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.38.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.32.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.0/512.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.3.2-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.38.0 trio-0.32.0 trio-websocket-0.12.2 wsproto-1.3.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "source": [
        "# --- User Input ---\n",
        "# This section prompts the user to provide the necessary information\n",
        "\n",
        "# Prompt the user for the LinkedIn job search URL.\n",
        "website_link = input(\"Enter the website link: \")\n",
        "if website_link.strip() == \"\":\n",
        "    # \"https://www.linkedin.com/jobs/search?keywords=Software%20Developer&location=United%20States&f_E=2%2C3&f_TPR=r2592000&position=1&pageNum=0\"\n",
        "    # \"https://www.linkedin.com/jobs/search/?currentJobId=4233045827&distance=25.0&f_E=2%2C3&f_TPR=r2592000&f_WT=1%2C3&keywords=software%20developer&origin=JOB_SEARCH_PAGE_JOB_FILTER\"\n",
        "    # \"https://www.linkedin.com/jobs/search?keywords=Software%20Development&location=United%20States&f_TPR=&f_E=2%2C3&position=1&pageNum=0\"\n",
        "    # \"https://www.linkedin.com/jobs/search?keywords=Software%20Engineer&location=United%20States&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0\"\n",
        "    # \"https://www.linkedin.com/jobs/search?keywords=Software%20Development&location=Greater%20Phoenix%20Area&f_JT=F&f_E=2&f_TPR=&f_WT=1%2C3&position=1&pageNum=0\"\n",
        "    # \"https://www.linkedin.com/jobs/search?keywords=Software%20Development&location=United%20States&geoId=103644278&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0\"\n",
        "    website_link = \"https://www.linkedin.com/jobs/search?keywords=Software%20Development&location=Greater%20Phoenix%20Area&f_JT=F&f_E=2&f_TPR=&f_WT=1%2C3&position=1&pageNum=0\"\n",
        "    print(f\"No website provided. Using default: {website_link}\")\n",
        "\n",
        "# Prompt the user for a base name to use for the output CSV and PDF files.\n",
        "name_files = input(\"Enter the name for the output files: \")\n",
        "if name_files.strip() == \"\":\n",
        "    name_files = \"linkedin_datascrpr_test\"\n",
        "    print(f\"No file name provided. Using default: {name_files}\")\n",
        "\n",
        "# Prompt the user for the maximum number of job listings to scrape.\n",
        "while True:\n",
        "    max_listings_input = input(\"Enter the maximum number of listings to scrape (as a number): \")\n",
        "    if max_listings_input.strip() == \"\":\n",
        "        max_listings = 100\n",
        "        print(f\"No input provided. Using default max listings: {max_listings}\")\n",
        "        break\n",
        "    try:\n",
        "        max_listings = int(max_listings_input)\n",
        "        if max_listings > 0:\n",
        "            break # Valid input, exit the loop\n",
        "        else:\n",
        "            print(\"Please enter a positive number.\")\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter a number.\")\n",
        "\n",
        "# Define a default wait time for page loading during scraping.This can be adjusted.\n",
        "wait_seconds = 2\n",
        "# wait_seconds = int(input(\"Enter the wait time in seconds (as a number): \"))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QPtJoPdg_A1",
        "outputId": "36197448-dbc9-4b8d-ae50-1798d87a8ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the website link: \n",
            "No website provided. Using default: https://www.linkedin.com/jobs/search?keywords=Software%20Development&location=Greater%20Phoenix%20Area&f_JT=F&f_E=2&f_TPR=&f_WT=1%2C3&position=1&pageNum=0\n",
            "Enter the name for the output files: \n",
            "No file name provided. Using default: linkedin_datascrpr_test\n",
            "Enter the maximum number of listings to scrape (as a number): \n",
            "No input provided. Using default max listings: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_data(name_file, website_link, post_limit, wait_time=1):\n",
        "    \"\"\"Scrapes job data from LinkedIn and saves it to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        name_file (str): The base name for the output CSV file.\n",
        "        website_link (str): The URL of the LinkedIn job search page.\n",
        "        post_limit (int): The maximum number of job postings to scrape.\n",
        "        wait_time (int, optional): The time in seconds to wait for elements to load. Defaults to 1.\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the scraped job data.\n",
        "    \"\"\"\n",
        "    # Initialize WebDriver with headless option for Colab environment\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "    # # Proxy ----------------------------------------------------------\n",
        "    # proxy_ip = \"159.69.57.20\"\n",
        "    # proxy_port = \"8880\"  # <<< REPLACE WITH THE ACTUAL PORT FROM PROXYSCRAPE\n",
        "    # proxy_server = f\"{proxy_ip}:{proxy_port}\"\n",
        "    # print(f\"Attempting to use proxy: {proxy_server}\") # Optional: Print the proxy being used\n",
        "    # chrome_options.add_argument(f'--proxy-server={proxy_server}')\n",
        "    # # ----------------------------------------------------------\n",
        "\n",
        "    try:\n",
        "        # Initialize the Chrome driver with the specified options\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "        # Navigate to the LinkedIn job search URL\n",
        "        driver.get(website_link)\n",
        "        # Send Escape Key Press to Clear Any Pop Ups (e.g., cookie banners)\n",
        "        webdriver.ActionChains(driver).send_keys(Keys.ESCAPE).perform()\n",
        "\n",
        "        for _ in range(10):\n",
        "          driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "          time.sleep(1)\n",
        "\n",
        "        try:\n",
        "            # Try to click the \"See more jobs\" button a few times\n",
        "            for _ in range(10): # Attempt up to X times\n",
        "                try:\n",
        "                    see_more_button = WebDriverWait(driver, 5).until(\n",
        "                        EC.element_to_be_clickable((By.CLASS_NAME, 'infinite-scroller__show-more-button'))\n",
        "                    )\n",
        "                    see_more_button.click()\n",
        "                    print(\"Clicked 'See more jobs' button.\")\n",
        "                    time.sleep(0.5) # Wait for content to load\n",
        "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "                except TimeoutException:\n",
        "                    print(\"No more 'See more jobs' button or it's not clickable.\")\n",
        "                    break # Exit the loop if the button is not found\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while trying to click 'See more jobs': {e}\")\n",
        "\n",
        "\n",
        "        job_data = []  # Initialize an empty list to store job data\n",
        "        count = 0\n",
        "\n",
        "        # # Find all job listing elements on the current page\n",
        "        job_listings = driver.find_elements(By.XPATH, '//section[@class=\"two-pane-serp-page__results-list\"]/ul/li')\n",
        "\n",
        "\n",
        "        # Loop through job listings up to the specified limit\n",
        "        while count < post_limit+1:\n",
        "            if not job_listings:\n",
        "                # Break the loop if no listings are found\n",
        "                print(f\"Error finding website\")\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # Get the current job listing element\n",
        "                listing = job_listings[count]\n",
        "                # Extract key information from the job listing summary\n",
        "                job_title = listing.find_element(By.CLASS_NAME, 'base-search-card__title').text\n",
        "                company = listing.find_element(By.CLASS_NAME, 'base-search-card__subtitle').text\n",
        "                address = listing.find_element(By.CLASS_NAME, 'job-search-card__location').text\n",
        "                date = listing.find_element(By.XPATH, '//*[starts-with(@class, \"job-search-card__listdate\")]').text\n",
        "                job_link = listing.find_element(By.CLASS_NAME, 'base-card__full-link').get_attribute('href')\n",
        "\n",
        "                # Open job details in a new tab to get the full description\n",
        "                driver.execute_script(\"window.open('');\")\n",
        "                # Switch to the new tab\n",
        "                driver.switch_to.window(driver.window_handles[-1])\n",
        "                # Navigate to the job link in the new tab\n",
        "                driver.get(job_link)\n",
        "                # Send Escape Key Press in the new tab to clear potential pop-ups\n",
        "                webdriver.ActionChains(driver).send_keys(Keys.ESCAPE).perform()\n",
        "\n",
        "                # Wait for the job description element to load\n",
        "                time.sleep(wait_time) # Using the passed wait_time\n",
        "\n",
        "                # Locate the job description section\n",
        "                # Locate the element containing the actual description text\n",
        "                description_element = driver.find_element(By.XPATH, '//*[@class=\"description__text description__text--rich\"]')\n",
        "\n",
        "                # Extract and prettify the HTML of the description\n",
        "                description_html = description_element.get_attribute('innerHTML')\n",
        "                pretty_html = BeautifulSoup(description_html, 'html.parser').prettify()\n",
        "\n",
        "                # Append the extracted data to the job_data list\n",
        "                job_data.append([job_title, company, date, address, job_link, pretty_html])\n",
        "\n",
        "                # Close the current tab and switch back to the main window\n",
        "                driver.close()\n",
        "                driver.switch_to.window(driver.window_handles[0])\n",
        "\n",
        "            except StaleElementReferenceException:\n",
        "                # Handle cases where the element is no longer available in the DOM\n",
        "                print(f\"StaleElementReferenceException occurred for listing {count}. Retrying...\")\n",
        "                continue  # Skip to the next iteration and try again\n",
        "            except TimeoutException:\n",
        "                # Handle cases where an element is not found within the wait time\n",
        "                print(f\"TimeoutException occurred for listing {count}. Element not found. Skipping this job.\")\n",
        "                continue  # Skip to the next iteration and try again\n",
        "            except Exception as e:\n",
        "                # Catch any other unexpected exceptions during scraping a single listing\n",
        "                print(f\"An unexpected error occurred for listing {count}: {e}. Skipping this job.\")\n",
        "                continue\n",
        "            finally:\n",
        "                # Increment the count regardless of whether an error occurred or not\n",
        "                count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any exceptions during the initial driver setup or page load\n",
        "        print(f\"An error occurred during the initial scraping setup or page load: {e}\")\n",
        "        # Return an empty DataFrame in case of initial errors\n",
        "        df = pd.DataFrame()\n",
        "\n",
        "    finally:\n",
        "        # Ensure the driver is quit even if errors occur\n",
        "        if 'driver' in locals() and driver:\n",
        "            driver.quit()\n",
        "\n",
        "    # Create a pandas DataFrame from the scraped data\n",
        "    df = pd.DataFrame(job_data, columns=[\"Job Title\", \"Company\", \"Date\", \"Address\", \"Link\", \"Description\"])\n",
        "\n",
        "    # Remove duplicate entries based on Job Title and Company\n",
        "    df.drop_duplicates(subset=[\"Job Title\", \"Company\"], keep=\"first\", inplace=True)\n",
        "\n",
        "    # Define the output CSV file path\n",
        "    output_csv_path = f\"{name_file}.csv\" # Removed hardcoded 'Files/'\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    try:\n",
        "        df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
        "        print(f\"Data saved to {output_csv_path}\")\n",
        "    except Exception as e:\n",
        "        # Handle potential errors during the file saving process\n",
        "        print(f\"Error saving data to CSV: {e}\")\n",
        "\n",
        "    # Return the DataFrame\n",
        "    return df\n",
        "\n",
        "# Execute the scraping function with user inputs\n",
        "# The wait_seconds variable is defined earlier from user input or defaults\n",
        "job_listings_df = scrape_data(name_files, website_link, max_listings, wait_time=wait_seconds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "id": "d0_i48FFeO-o",
        "outputId": "626d6480-6206-4858-ceb9-398f3c9cae48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred during the initial scraping setup or page load: Message: session not created: Chrome instance exited. Examine ChromeDriver verbose log to determine the cause.; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#sessionnotcreatedexception\n",
            "Stacktrace:\n",
            "#0 0x5ac414e344ca <unknown>\n",
            "#1 0x5ac414882e4b <unknown>\n",
            "#2 0x5ac4148bd919 <unknown>\n",
            "#3 0x5ac4148b9375 <unknown>\n",
            "#4 0x5ac414909fe6 <unknown>\n",
            "#5 0x5ac414909706 <unknown>\n",
            "#6 0x5ac4148c7c2a <unknown>\n",
            "#7 0x5ac4148c8931 <unknown>\n",
            "#8 0x5ac414dfacf9 <unknown>\n",
            "#9 0x5ac414dfdcdc <unknown>\n",
            "#10 0x5ac414de3f79 <unknown>\n",
            "#11 0x5ac414dfe8b5 <unknown>\n",
            "#12 0x5ac414dcb9c3 <unknown>\n",
            "#13 0x5ac414e21228 <unknown>\n",
            "#14 0x5ac414e21403 <unknown>\n",
            "#15 0x5ac414e33463 <unknown>\n",
            "#16 0x7bbd6bfd4ac3 <unknown>\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'job_data' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-826806489.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;31m# Execute the scraping function with user inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;31m# The wait_seconds variable is defined earlier from user input or defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m \u001b[0mjob_listings_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwebsite_link\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_listings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-826806489.py\u001b[0m in \u001b[0;36mscrape_data\u001b[0;34m(name_file, website_link, post_limit, wait_time)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# Create a pandas DataFrame from the scraped data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Job Title\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Company\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Address\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Link\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Description\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Remove duplicate entries based on Job Title and Company\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'job_data' where it is not associated with a value"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Initialize the list of keywords to identify requirement sections\n",
        "keywords = [\n",
        "    \"requirements\", \"requirement\", \"qualifications\", \"qualification\", \"required education and experience\",\n",
        "    \"skills/qualifications\", \"skills/qualification\", \"must have\", \"required skills\", \"desired skills\",\n",
        "    \"preferred qualifications\", \"preferred qualification\", \"basic qualifications\", \"basic qualification\",\n",
        "    \"essential skills\", \"essential skill\", \"job requirements\", \"job requirement\", \"job qualifications\",\n",
        "    \"job qualification\", \"required experience\", \"preferred experience\", \"required education\", \"desired qualifications\",\n",
        "    \"desired qualification\", \"experience\", \"education\", \"skills\", \"skill\", \"must-have\", \"must haves\", \"must-have skills\",\n",
        "    \"must-have skill\", \"required\", \"preferred\", \"essential\", \"basic\", \"necessary\", \"mandatory\", \"needed\", \"desired\",\n",
        "    \"Who Should Apply\", \"What We're Looking For\", \"What You'll Do\", \"What You'll Need\",\n",
        "    \"What You'll Bring\", \"What You'll Be Doing\", \"What You'll Be Responsible For\", \"What do you need to have\",\n",
        "    \"What Do You Need To Bring\", \"What You'll Need To Bring\", \"What You'll Need To Have\", \"What You'll Need To Do\",\n",
        "    \"you'll use\", \"Nice To Have\",\n",
        "]"
      ],
      "metadata": {
        "id": "HJqjlEBdpp5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Compile a regular expression pattern to match any of the keywords\n",
        "keywords_pattern = re.compile('|'.join([re.escape(keyword) for keyword in keywords]), re.IGNORECASE)\n",
        "\n",
        "# Function to check if a string contains any of the keywords\n",
        "def contains_keywords(text):\n",
        "    return bool(keywords_pattern.search(text))\n",
        "\n",
        "def not_single_word(text):\n",
        "    return ' ' in text\n",
        "\n",
        "def clean_html(description):\n",
        "    soup = BeautifulSoup(description, 'html.parser')\n",
        "    text = soup.get_text(separator=\"\\n\")\n",
        "    # Remove leading/trailing whitespaces and empty lines\n",
        "    cleaned_text = \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
        "    return cleaned_text\n",
        "\n",
        "def extract_csv(df):\n",
        "    modified_rows = []\n",
        "    all_requirements = []\n",
        "    fieldnames = df.columns.tolist() + ['Cleaned'] + ['Requirements']\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        collected_text = []\n",
        "\n",
        "        collected_text.append(row['Job Title']+\" \"+row['Company']+\" \"+row['Date']+\" \"+row['Address'])\n",
        "        description = row['Description']\n",
        "        soup = BeautifulSoup(description, 'html.parser')\n",
        "\n",
        "        strong_elements = soup.find_all('strong')\n",
        "        i = 0\n",
        "        for strong in strong_elements:\n",
        "\n",
        "            if contains_keywords(strong.get_text(strip=True)):\n",
        "                i += 1\n",
        "                collected_text.append(\"\\n\" + str(i) + \"\\n\")\n",
        "                collected_text.append(strong.get_text(strip=True))\n",
        "\n",
        "                for sibling in strong.next_siblings:\n",
        "                    if sibling.name == 'strong':\n",
        "                        break\n",
        "                    # if sibling.name in ['p', 'li', 'br', 'span']:\n",
        "                    #     text1 = sibling.get_text().strip()\n",
        "                    #     if text1:\n",
        "                    #         collected_text.append(\"     #######     \"+text1)\n",
        "                    elif isinstance(sibling, str):\n",
        "                        text2 = sibling.strip()\n",
        "                        if text2:\n",
        "                            collected_text.append(text2)\n",
        "\n",
        "                for sibling in strong.find_all_next():\n",
        "                    if sibling.name == 'strong':\n",
        "                        break\n",
        "                    if sibling.name in ['p', 'li', 'br', 'span']:\n",
        "                        text1 = sibling.get_text(strip=True)\n",
        "                        if text1:\n",
        "                            collected_text.append(text1)\n",
        "\n",
        "        li_elements = soup.find_all('li')\n",
        "        for li in li_elements:\n",
        "            collected_text.append(li.get_text(strip=True))\n",
        "\n",
        "        collected_text.append(\"\\n__________________________________________________\\n\")\n",
        "\n",
        "        seen = set()\n",
        "        unique_collected_text = []\n",
        "        for section in collected_text:\n",
        "            if section not in seen:\n",
        "                unique_collected_text.append(section)\n",
        "                seen.add(section)\n",
        "\n",
        "        row_dict = row.to_dict()\n",
        "        row_dict['Cleaned'] = clean_html(row_dict['Description'])\n",
        "        row_dict['Requirements'] = \"\\n\".join(unique_collected_text)\n",
        "        all_requirements.extend(unique_collected_text)\n",
        "        modified_rows.append(row_dict)\n",
        "\n",
        "    modified_df = pd.DataFrame(modified_rows)\n",
        "    return modified_df, all_requirements\n",
        "\n",
        "def download_data(name_file, df):\n",
        "    modified_df, all_requirements = extract_csv(df)\n",
        "    modified_df.to_csv(name_file + '.csv', index=False, encoding='utf-8')\n",
        "\n",
        "    requirements = all_requirements\n",
        "    requirements = [req for req in requirements if req.strip()]\n",
        "\n",
        "    with open(name_file + '_reqs.txt', 'w', encoding='utf-8') as file:\n",
        "        for req in requirements:\n",
        "            req = req.replace('\\n', '')\n",
        "            print(req)\n",
        "            file.write(req+\"\\n\")\n",
        "\n",
        "download_data(name_files, job_listings_df)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ml_9XKcpJs-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Initialize the set of custom stop words for text analysis\n",
        "custom_stop_words = set([\n",
        "        \"year\", \"years\", \"work\", \"working\", \"use\", \"using\", \"new\", \"understand\",\n",
        "        \"knowledge\", \"skills\", \"ability\", \"must\", \"have\", \"with\", \"the\", \"and\",\n",
        "        \"or\", \"to\", \"a\", \"in\", \"of\", \"for\", \"on\", \"as\", \"is\", \"are\", \"understanding\", \"experience\",\n",
        "        \"software\", \"development\", \"developer\", \"technologies\", \"technology\",\n",
        "        \"strong\", \"design\", \"business\", \"computer\",\"degree\", \"related\", \"environment\", \"solution\",\n",
        "        \"project\", \"engineering\", \"technical\", \"system\", \"application\", \"applications\",\n",
        "        \"solutions\", \"preferred\", \"data\", \"problem\", \"excellent\", \"plus\", \"practice\",\n",
        "        \"skill\", \"team\", \"teams\", \"support\", \"science\", \"life\", \"basic\", \"best\",\n",
        "        \"within\", \"security\", \"required\", \"approach\",\"role\", \"service\", \"services\", \"field\",\n",
        "        \"familiarity\", \"equivalent\", \"learn\", \"industry\", \"build\", \"building\",\n",
        "        \"academic\", \"cumulative\", \"gpa\", \"company\", \"companies\", \"government\",\n",
        "        \"agency\", \"agencies\", \"perform\", \"performing\", \"performs\",\"text\",\"week\",\n",
        "        \"tech\", \"clients\", \"client\", \"skillstorm\", \"built\", \"paid\", \"enterprise\",\n",
        "        \"oriented\", \"user\", \"requirement\", \"requirements\", \"professional\",\n",
        "        \"management\", \"information\", \"cycle\", \"proficiency\", \"methodology\",\n",
        "        \"verbal\", \"proficient\", \"one\", \"platform\", \"platforms\",\"opportunities\",\n",
        "        \"across\", \"interview\", \"interviews\", \"professionals\",\"impact\",\"modern\",\"youll\",\n",
        "        \"office\", \"developing\", \"level\", \"training\", \"partner\", \"partners\",\n",
        "        \"position\", \"equal\", \"greater\", \"open\", \"language\", \"languages\",\"call\",\"right\",\"key\",\n",
        "        \"procedure\", \"procedures\", \"delivery\", \"general\", \"issue\", \"issues\", \"and/or\",\n",
        "        \"2\", \"hands-on\", \"etc.\", \"on-site\", \"framework\", \"frameworks\",\n",
        "        \"coding\", \"time\", \"ii\", \"configuration\", \"configurations\", \"product\", \"products\", \"scripting\",\n",
        "        \"provide\", \"providing\", \"production\", \"implement\", \"implementing\", \"code\",\n",
        "        \"perform\", \"career\", \"contribute\", \"contributing\", \"selected\",\"high\",\"drive\",\n",
        "        \"demonstrate\", \"demonstrated\", \"develop\", \"developing\", \"desire\",\n",
        "        \"previous\", \"pattern\", \"effectively\", \"salary\", \"compensation\",\"phoenix\",\"message\",\"kforce\",\n",
        "        \"benefit\", \"benefits\", \"combination\", \"similar\", \"end\", \"expertise\",\"scottsdale\",\"receive\",\"rate\",\n",
        "        \"like\", \"good\", \"core\", \"spring\", \"slack\", \"e.g.\", \"highly\", \"performance\",\n",
        "        \"candidate\", \"job\", \"feature\", \"features\", \"remote\", \"program\", \"programs\", \"quality\",\n",
        "        \"insurance\", \"get\", \"engineer\", \"engineers\", \"master\", \"opportunity\",\n",
        "        \"employee\", \"employees\", \"please\", \"take\", \"u\", \"review\", \"flexible\",\n",
        "        \"manager\", \"managers\", \"maintain\", \"maintaining\", \"synergisticit\", \"electrical\",\n",
        "        \"minimum\", \"help\", \"helping\", \"infrastructure\",\"deployment\", \"deployments\", \"boot\",\n",
        "        \"qualification\", \"qualifications\", \"leave\", \"participate\", \"participating\",\n",
        "        \"supporting\", \"based\", \"startup\", \"startups\", \"comprehensive\",\n",
        "        \"world\", \"continuously\", \"make\", \"making\", \"share\", \"sharing\", \"growth\", \"ownership\",\n",
        "        \"implementation\", \"implementations\", \"mobile\", \"looking\", \"existing\", \"web\",\n",
        "        \"jobseekers\", \"customer\", \"customers\", \"someone\", \"love\", \"important\", \"helped\", \"care\",\n",
        "        \"fully\", \"know\", \"market\", \"markets\", \"assist\", \"assisting\",\n",
        "        \"company-paid\", \"history\", \"change\", \"changes\", \"part\", \"standard\", \"standards\",\n",
        "        \"assigned\", \"specification\", \"specifications\", \"document\", \"documents\",\n",
        "        \"health\", \"apply\", \"applying\", \"schedule\", \"schedules\",\"effective\",\"account\",\n",
        "        \"stakeholder\", \"stakeholders\", \"pay\", \"functional\", \"relevant\",\n",
        "        \"education\", \"create\", \"creating\", \"clearance\", \"without\", \"also\", \"creating\", \"per\",\n",
        "        \"task\", \"tasks\", \"collaborative\", \"hour\", \"hours\", \"united\",\"large\",\"concept\",\n",
        "        \"duty\", \"duties\", \"relocation\", \"sponsorship\", \"state\", \"states\",\n",
        "        \"control\", \"controlling\", \"manage\", \"managing\", \"dental\", \"closely\", \"medical\", \"email\",\n",
        "        \"organization\", \"organizations\", \"scope\", \"day\", \"ensuring\", \"visa\",\n",
        "        \"excellence\", \"specific\", \"solve\", \"lead\", \"leading\", \"keep\",\n",
        "        \"global\", \"internal\", \"external\", \"member\", \"members\", \"able\",\n",
        "        \"request\", \"requests\", \"various\", \"operational\", \"attention\",\"space\",\"relate\",\"levelfull\",\n",
        "        \"disability\", \"responsibility\", \"responsibilities\", \"operation\", \"operations\",\n",
        "        \"department\", \"departments\", \"operating\", \"employer\", \"location\", \"locations\", \"source\", \"etl\",\n",
        "        \"offer\", \"offering\", \"levelentry\", \"employment\", \"typefull-timejob\", \"principle\",\n",
        "        \"principles\", \"include\", \"including\", \"write\", \"writing\",\"boston\",\"would\",\"ago\",\"az\",\n",
        "        \"range\", \"base\", \"utilize\", \"utilizing\", \"deliver\", \"delivering\", \"gain\",\n",
        "        \"contributor\", \"contributors\", \"drive\", \"driving\", \"foster\", \"fostering\",\n",
        "        \"navigate\", \"navigating\", \"optimize\", \"optimizing\", \"gaining\", \"leverage\", \"leveraging\",\n",
        "        \"troubleshoot\", \"troubleshooting\", \"validate\", \"validating\", \"verify\",\n",
        "        \"monitor\", \"monitoring\", \"guide\", \"guiding\", \"mentor\", \"mentoring\", \"collaborate\",\n",
        "        \"collaborating\", \"communicate\", \"communicating\",\"verifying\", \"usd\",\n",
        "        \"improve\", \"eg\", \"require\", \"least\",\"prefer\", \"game\",\n",
        "        \"seniority\", \"dynamic\",\"e\",\"g\",\"etc\",\"assess\", \"assessing\", \"evaluate\", \"evaluating\",\n",
        "        \"practices\", \"process\", \"processes\", \"including\", \"tool\", \"tools\", \"written\",\n",
        "        \"vision\", \"stack\", \"person\", \"complex\", \"well\", \"result\", \"results\", \"need\", \"needed\", \"analytical\"\n",
        "        \"status\", \"eligible\", \"ensure\", \"ensuring\", \"may\", \"area\", \"areas\", \"vehicle\",\n",
        "        \"meet\", \"meeting\", \"u.s.\", \"plan\", \"plans\", \"appropriate\", \"success\",\n",
        "        \"policy\", \"policies\", \"necessary\", \"conduct\", \"conducting\", \"workflow\", \"workflows\",\n",
        "        \"keeping\", \"detail\", \"details\", \"identify\", \"identifying\", \"continuous\", \"power\",\n",
        "        \"grow\",\"equity\",\"san\",\"francisco\",\"multiple\",\"search\",\"people\",\"bonus\",\"stay\",\n",
        "        \"passion\",\"parental\",\"401k\",\"ca\",\"status\",\"match\",\"pto\",\"york\",\"comfortable\",\n",
        "        \"holiday\",\"find\",\"follow\",\"link\", \"palo\",\"alto\", \"action\",\"believe\",\n",
        "        \"race\",\"gender\",\"color\",\"identify\", \"origin\", \"national\", \"protect\",\"veteran\",\n",
        "        \"orientation\",\"regard\",\"sexual\",\"identity\",\"age\",\"genetic\",\"military\",\n",
        "\n",
        "    ])\n",
        "\n",
        "# Add User Input Words to Remove"
      ],
      "metadata": {
        "id": "qBJ_h_yopzv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "# Function to read requirements from the file\n",
        "# Modified to read from a DataFrame instead\n",
        "def read_requirements(df):\n",
        "    \"\"\"\n",
        "    Combines the 'Requirements' column from a DataFrame into a single string.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing job data with a 'Requirements' column.\n",
        "\n",
        "    Returns:\n",
        "        str: A single string containing all combined requirements.\n",
        "    \"\"\"\n",
        "    data = df['Requirements'].str.cat()\n",
        "    return '\\n'.join(data.split(\"\\n__________________________________________________\\n\"))\n",
        "\n",
        "# Function to combine similar words\n",
        "def combine_similar_words(tokens, word1, word2):\n",
        "    \"\"\"\n",
        "    Replaces occurrences of word2 with word1 in a list of tokens.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): A list of words (tokens).\n",
        "        word1 (str): The word to replace with.\n",
        "        word2 (str): The word to be replaced.\n",
        "\n",
        "    Returns:\n",
        "        list: A new list of tokens with replacements.\n",
        "    \"\"\"\n",
        "    combined_tokens = []\n",
        "    for token in tokens:\n",
        "        if token == word1 or token == word2:\n",
        "            combined_tokens.append(word1)\n",
        "        else:\n",
        "            combined_tokens.append(token)\n",
        "    return combined_tokens\n",
        "\n",
        "# Frequency Analysis\n",
        "def frequency_analysis(tokens, num_top, pdf):\n",
        "    \"\"\"\n",
        "    Performs frequency analysis on tokens, generates bar and pie charts,\n",
        "    and updates the global top_skills list.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): A list of tokens for analysis.\n",
        "        num_top (int): The number of top frequent items to display.\n",
        "        pdf (PdfPages): The PDF object to save plots to.\n",
        "    \"\"\"\n",
        "\n",
        "    # Count the frequency of each token\n",
        "    frequency = Counter(tokens)\n",
        "    # Get the most common tokens\n",
        "    most_common = frequency.most_common(num_top)\n",
        "    words, counts = zip(*most_common)\n",
        "\n",
        "    # Store the top most frequent tokens in the global list\n",
        "    top_skills = list(words)\n",
        "\n",
        "    # Generate and save Bar Graph\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(words, counts)\n",
        "    plt.xlabel('Skills')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title(f'Top {num_top} Most Common Skills in Requirements (Frequency)')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    pdf.savefig()  # Save the current figure to the PDF\n",
        "    plt.show()\n",
        "    return top_skills\n",
        "\n",
        "\n",
        "# Custom color function for word clouds\n",
        "def random_color_func(word=None, font_size=None, position=None, orientation=None, random_state=None, **kwargs):\n",
        "    \"\"\"Generates a random HSL color.\"\"\"\n",
        "    return \"hsl({}, {}%, {}%)\".format(random.randint(0, 360), random.randint(50, 100), random.randint(25, 75))\n",
        "\n",
        "# Word Cloud\n",
        "def generate_word_cloud(text, title, pdf):\n",
        "    \"\"\"\n",
        "    Generates a word cloud from the given text and saves it to a PDF.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to generate the word cloud from.\n",
        "        title (str): The title for the word cloud plot.\n",
        "        pdf (PdfPages): The PDF object to save the plot to.\n",
        "    \"\"\"\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', color_func=random_color_func).generate(text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off') # Hide axes\n",
        "    plt.title(title)\n",
        "    pdf.savefig()  # Save the current figure to the PDF\n",
        "    plt.show()\n",
        "\n",
        "# N-gram Analysis\n",
        "def ngram_analysis(tokens, n, num_top, pdf):\n",
        "    \"\"\"\n",
        "    Performs N-gram analysis, generates a pie chart, and saves it to a PDF.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): A list of tokens for N-gram analysis.\n",
        "        n (int): The size of the N-grams.\n",
        "        num_top (int): The number of top N-grams to display.\n",
        "        pdf (PdfPages): The PDF object to save the plot to.\n",
        "    \"\"\"\n",
        "    # Generate N-grams\n",
        "    n_grams = ngrams(tokens, n)\n",
        "    # Count the frequency of each N-gram\n",
        "    ngram_frequency = Counter(n_grams)\n",
        "    # Get the most common N-grams\n",
        "    most_common = ngram_frequency.most_common(num_top)\n",
        "\n",
        "    if not most_common:\n",
        "        print(f\"No {n}-grams found to analyze.\")\n",
        "        return\n",
        "\n",
        "    ngrams_list, counts = zip(*most_common)\n",
        "    # Create labels for the pie chart\n",
        "    ngrams_labels = [' '.join(ngram) for ngram in ngrams_list]\n",
        "\n",
        "    # # Generate and save Pie Chart for N-grams\n",
        "    # plt.figure(figsize=(10, 5))\n",
        "    # plt.pie(counts, labels=[f'{label} ({count})' for label, count in zip(ngrams_labels, counts)], autopct='%1.1f%%',\n",
        "    #         startangle=140)\n",
        "    # plt.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "    # plt.title(f'Top {num_top} Most Common {n}-grams in Requirements')\n",
        "    # pdf.savefig()  # Save the current figure to the PDF\n",
        "    # plt.show()\n",
        "\n",
        "    # --- Generate and save Bar Graph for N-grams ---\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(ngrams_labels, counts)\n",
        "    plt.xlabel(f'{n}-grams')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title(f'Top {num_top} Most Common {n}-grams in Requirements (Bar Graph)')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    pdf.savefig()  # Save the current figure to the PDF\n",
        "    plt.show()\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    \"\"\"\n",
        "    Maps an NLTK part-of-speech tag to a WordNet part-of-speech tag.\n",
        "\n",
        "    Args:\n",
        "        tag (str): The NLTK part-of-speech tag.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The corresponding WordNet tag ('a', 'v', 'n', 'r') or None.\n",
        "    \"\"\"\n",
        "    if tag.startswith('J'):\n",
        "        return 'a' # Adjective\n",
        "    elif tag.startswith('V'):\n",
        "        return 'v' # Verb\n",
        "    elif tag.startswith('N'):\n",
        "        return 'n' # Noun\n",
        "    elif tag.startswith('R'):\n",
        "        return 'r' # Adverb\n",
        "    else:\n",
        "        return None # Default to noun if not found\n",
        "\n",
        "def get_tfidf_features(df):\n",
        "    \"\"\"\n",
        "    Calculates TF-IDF features for the 'Requirements' column in the DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing job data with a 'Requirements' column.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - tfidf_matrix (sparse matrix): The TF-IDF matrix.\n",
        "            - feature_names (list): The list of terms (features) from the vectorizer.\n",
        "    \"\"\"\n",
        "    if df.empty or 'Requirements' not in df.columns:\n",
        "        print(\"DataFrame is empty or does not have a 'Requirements' column for TF-IDF analysis.\")\n",
        "        return None, None\n",
        "\n",
        "    # We need a list of strings, where each string is the processed requirements\n",
        "    # for a single job listing. Let's re-process the requirements for TF-IDF.\n",
        "    processed_descriptions = []\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    all_stop_words = stop_words.union(custom_stop_words)\n",
        "\n",
        "\n",
        "    for description in df['Requirements']:\n",
        "        if pd.notna(description): # Check if description is not NaN\n",
        "            tokens = word_tokenize(description)\n",
        "            lemmatized_and_filtered_tokens = []\n",
        "            pos_tags = nltk.pos_tag(tokens)\n",
        "            for word, tag in pos_tags:\n",
        "                 wordnet_tag = get_wordnet_pos(tag)\n",
        "                 if wordnet_tag is None:\n",
        "                    lemma = lemmatizer.lemmatize(word.lower())\n",
        "                 else:\n",
        "                    lemma = lemmatizer.lemmatize(word.lower(), wordnet_tag)\n",
        "\n",
        "                 # More robust filtering: ensure it has letters and not a stop word after lemmatization\n",
        "                 if re.search(r'[a-zA-Z]', lemma) and lemma not in all_stop_words:\n",
        "                    lemmatized_and_filtered_tokens.append(lemma)\n",
        "\n",
        "            processed_descriptions.append(\" \".join(lemmatized_and_filtered_tokens))\n",
        "        else:\n",
        "             processed_descriptions.append(\"\") # Add empty string for missing descriptions\n",
        "\n",
        "    if not processed_descriptions:\n",
        "        print(\"No processed descriptions available for TF-IDF.\")\n",
        "        return None, None\n",
        "\n",
        "    # Initialize TF-IDF Vectorizer\n",
        "    # You can adjust max_features and other parameters as needed\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=200, stop_words=list(all_stop_words)) # Added stop_words here as well\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(processed_descriptions)\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "    return tfidf_matrix, feature_names\n",
        "\n",
        "# Function to plot TF-IDF scores\n",
        "def plot_tfidf_bar_graph(tfidf_matrix, feature_names, num_top, pdf):\n",
        "    \"\"\"\n",
        "    Plots a bar graph of the top TF-IDF features.\n",
        "\n",
        "    Args:\n",
        "        tfidf_matrix (sparse matrix): The TF-IDF matrix.\n",
        "        feature_names (list): The list of terms (features) from the vectorizer.\n",
        "        num_top (int): The number of top TF-IDF features to display.\n",
        "        pdf (PdfPages): The PDF object to save the plot to.\n",
        "    \"\"\"\n",
        "    if tfidf_matrix is None or feature_names is None:\n",
        "        print(\"No TF-IDF data available to plot.\")\n",
        "        return\n",
        "\n",
        "    # Sum the TF-IDF scores across all documents for each feature\n",
        "    sum_tfidf = tfidf_matrix.sum(axis=0)\n",
        "\n",
        "    # Get the feature names and their summed TF-IDF scores\n",
        "    tfidf_scores = [(feature_names[i], sum_tfidf[0, i]) for i in range(len(feature_names))]\n",
        "\n",
        "    # Sort the features by their summed TF-IDF scores in descending order\n",
        "    tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get the top N features and their scores\n",
        "    top_tfidf_features = tfidf_scores[:num_top]\n",
        "    words, scores = zip(*top_tfidf_features)\n",
        "\n",
        "    # Generate and save Bar Graph for TF-IDF\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(words, scores)\n",
        "    plt.xlabel('Skills/Terms')\n",
        "    plt.ylabel('Total TF-IDF Score')\n",
        "    plt.title(f'Top {num_top} Most Important Terms in Requirements (TF-IDF)')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    pdf.savefig()  # Save the current figure to the PDF\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Analyze Data\n",
        "def analyze_data(name_file, df):\n",
        "    \"\"\"\n",
        "    Analyzes job requirements from a DataFrame and generates visualizations.\n",
        "\n",
        "    Args:\n",
        "        name_file (str): The base name for output files.\n",
        "        df (pd.DataFrame): The DataFrame containing job data with a 'Requirements' column.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        print(\"DataFrame is empty. No data to analyze.\")\n",
        "        return\n",
        "\n",
        "    # Load requirements from the DataFrame\n",
        "    # Combine all requirements into a single string\n",
        "    all_requirements_combined = read_requirements(df)\n",
        "    all_requirements_combined = all_requirements_combined.lower()\n",
        "    all_requirements_combined = re.sub(r'[&`;:!@^{}/()=~<>\\\"]', ' ', all_requirements_combined)\n",
        "    all_requirements_combined =  re.sub(r\"[\\'.,_\\\\]\", '', all_requirements_combined)\n",
        "    all_requirements_combined = all_requirements_combined.replace(\"machine learning\",\"machinelearning\")\n",
        "    all_requirements_combined = all_requirements_combined.replace('c plus plus','c++')\n",
        "    all_requirements_combined = all_requirements_combined.replace('artificial intelligence','ai')\n",
        "    all_requirements_combined = all_requirements_combined.replace('data science','datascience')\n",
        "    all_requirements_combined = all_requirements_combined.replace('back end','backend')\n",
        "    all_requirements_combined = all_requirements_combined.replace('front end','frontend')\n",
        "    all_requirements_combined = all_requirements_combined.replace('full stack','full-stack')\n",
        "    all_requirements_combined = all_requirements_combined.replace('ci cd','ci-cd')\n",
        "    all_requirements_combined = all_requirements_combined.replace('ui ux','ui-ux')\n",
        "    all_requirements_combined = all_requirements_combined.replace('version control','version-control')\n",
        "    all_requirements_combined = all_requirements_combined.replace('unit testing','unit-testing')\n",
        "    all_requirements_combined = all_requirements_combined.replace('unit test','unit-testing')\n",
        "    all_requirements_combined = all_requirements_combined.replace('unit integration','unit-integration')\n",
        "    all_requirements_combined = all_requirements_combined.replace('full scale','full-scale')\n",
        "    all_requirements_combined = all_requirements_combined.replace('full coverage','full-coverage')\n",
        "    all_requirements_combined = all_requirements_combined.replace('react native','react-native')\n",
        "    all_requirements_combined = all_requirements_combined.replace('restful apis','restful-api')\n",
        "    all_requirements_combined = all_requirements_combined.replace('restful api','restful-api')\n",
        "    all_requirements_combined = all_requirements_combined.replace('version control','version-control')\n",
        "    all_requirements_combined = all_requirements_combined.replace('problem solving','problem-solving')\n",
        "    all_requirements_combined = all_requirements_combined.replace('relational database','relational-database')\n",
        "\n",
        "    if not all_requirements_combined.strip():\n",
        "        print(\"No requirements found in the data for analysis.\")\n",
        "        return\n",
        "\n",
        "    # Tokenize the combined text\n",
        "    tokens = word_tokenize(all_requirements_combined)\n",
        "\n",
        "    # Combine similar words to normalize frequency counts\n",
        "    tokens = combine_similar_words(tokens, 'front-end', 'frontend')\n",
        "    tokens = combine_similar_words(tokens, 'back-end', 'backend')\n",
        "    tokens = combine_similar_words(tokens, 'apis', 'api')\n",
        "    tokens = combine_similar_words(tokens, 'windows', 'window')\n",
        "    tokens = combine_similar_words(tokens, 'analysis', 'analyze')\n",
        "    tokens = combine_similar_words(tokens, 'analysis', 'analytics')\n",
        "    tokens = combine_similar_words(tokens, 'reporting', 'report')\n",
        "    tokens = combine_similar_words(tokens, 'communication', 'communicate')\n",
        "    tokens = combine_similar_words(tokens, 'oop', 'object-oriented')\n",
        "    tokens = combine_similar_words(tokens, 'oop', 'objectoriented')\n",
        "    tokens = combine_similar_words(tokens, 'machine-learning', 'machinelearning')\n",
        "    tokens = combine_similar_words(tokens,'ci-cd','cicd')\n",
        "    tokens = combine_similar_words(tokens,'full-coverage','fullcoverage')\n",
        "\n",
        "    # Lemmatization and remove purely numeric tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = []\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    for word, tag in pos_tags:\n",
        "        wordnet_tag = get_wordnet_pos(tag)\n",
        "        if wordnet_tag is None:\n",
        "            lemma = lemmatizer.lemmatize(word.lower())\n",
        "        else:\n",
        "            lemma = lemmatizer.lemmatize(word.lower(), wordnet_tag)\n",
        "        if not lemma.isdigit() and re.search(r'[a-zA-Z]', lemma) and len(lemma) > 1:\n",
        "            lemmatized_tokens.append(lemma)\n",
        "\n",
        "    # Remove stopwords and custom words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    all_stop_words = stop_words.union(custom_stop_words)\n",
        "\n",
        "    # Filter tokens: keep words with at least one alphanumeric character and not in stopwords\n",
        "    filtered_tokens = [word for word in lemmatized_tokens if word not in all_stop_words and re.search(r'\\w', word) and word.strip()]\n",
        "\n",
        "    # --- TF-IDF Calculation ---\n",
        "    tfidf_matrix, feature_names = get_tfidf_features(df)\n",
        "\n",
        "    # Save all plots to a single PDF file\n",
        "    try:\n",
        "        with PdfPages(name_file + '_analysis.pdf') as pdf:\n",
        "            top_skills = frequency_analysis(filtered_tokens, 50, pdf)\n",
        "\n",
        "            # Plot TF-IDF bar graph if TF-IDF data is available\n",
        "            if tfidf_matrix is not None and feature_names is not None:\n",
        "                 plot_tfidf_bar_graph(tfidf_matrix, feature_names, 50, pdf)\n",
        "            else:\n",
        "                 print(\"Skipping TF-IDF plot as no data was generated.\")\n",
        "\n",
        "            ngram_analysis(filtered_tokens, 2, 30, pdf)\n",
        "            ngram_analysis(filtered_tokens, 3, 30, pdf)\n",
        "            ngram_analysis(filtered_tokens, 4, 30, pdf)\n",
        "            ngram_analysis(filtered_tokens, 6, 30, pdf)\n",
        "\n",
        "            # top_skills is populated by frequency_analysis\n",
        "            if top_skills: # Ensure top_skills is not empty\n",
        "                generate_word_cloud(' '.join(top_skills), 'Word Cloud of Top Skills', pdf)\n",
        "            else:\n",
        "                print(\"Skipping Top Skills Word Cloud as no top skills were identified.\")\n",
        "\n",
        "        print(f\"Analysis complete. Plots saved to {name_file}_analysis.pdf\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during plot generation or saving: {e}\")\n",
        "\n",
        "\n",
        "# Load data from the CSV file\n",
        "try:\n",
        "    df = pd.read_csv(name_files+'.csv')\n",
        "    print(f\"Successfully loaded data from {name_files}.csv\")\n",
        "    # Perform data analysis\n",
        "    analyze_data(name_files, df)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {name_files}.csv was not found. Please ensure the scraping step completed successfully.\")\n",
        "except pd.errors.EmptyDataError:\n",
        "    print(f\"Error: The file {name_files}.csv is empty. No data to analyze.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while loading or analyzing the data: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gnUBdJn6lGHY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}